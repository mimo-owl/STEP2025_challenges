# Documentation for Week2 Challenges
### Author: Mimo Shirasaka

## Challenge 1
最も良いプログラム：hash_table2.py（再ハッシュと collision対策入りの hash table）<br>
その他プログラム：hash_table.py（ベーシックな hash table）<br>

* 以下の説明は hash_table4.pyについて書きます。
1. Delete()関数の実装
    1. マップ内の探す箱（bucket）の住所を算出：
        削除したい単語`key`に対し、ハッシュ値を計算。次にマップの長さ`bucket_size`で modをとる。
    2. 探す箱の中に`key`と一致する単語（アイテム）があるかを探す：

        - 見つかったら<br>
            -> 1つ前のマス`prev`が指すポインタ`prev.next`を、今いるマス`item`ではなく、今いる次のマス`item.next`を指すようにすることで今いるマスを実質的に削除(delete)する<br>
            -> `return True`<br>
        - 見つからなかったら<br>
            -> 次を確認<br>
        - 最後まで見つからなかったら<br>
            -> `return False`<br>
2. 再ハッシュの実装<br>
    1. 再ハッシュの条件をつける：<br>
        要素数がテーブルサイズの 70% を上回ったら、テーブルサイズを 2 倍に拡張、要素数がテーブルサイズの 30% を下回ったら、テーブルサイズを半分に縮小<br>
    2. ハッシュテーブルを resize する：<br>
        新しいテーブル`new_buckets`を resize するサイズで用意し、元のテーブルを参照。bucketごとに見ていき、連結リストで繋がれた各アイテムについて`new_buckets`用の住所を算出する。
        各アイテムについて`Item()`を用いてリフォオーマットし、新しいテーブルに格納する。<br>
    3. 再ハッシュ済みの新しいテーブル`new_buckets`を、現在のテーブル`self.buckets`として定義する。<br>

3. Collision 対策<br>
    サンプルコードにおけるハッシュ値算出の課題は、AliceとElicaなど、文字の組み合わせが同じものが存在する時に、bucket住所が同じになり、衝突(collision) が発生する、というものであった。
    サンプルコードでは、名前に含まれる文字のASCIIコードの和をハッシュ値として計算しており、文字の登場順番が考慮されていなかった。
    そこで、Collision対策として、文字の登場順番を考慮したハッシュ値を算出すればより計算効率が速くなるのではないかと考えた。
    実装としては、最初の文字を`i=0`番目として、`(i + 1) * \<文字のASCIIコード\>` の和をハッシュ値とすることにより順番が違うと違う bucket住所になるようにした。

## Challenge 2
### 問題
木構造を使えば O(log N)、ハッシュテーブルを使えばほぼ O(1) で検索・追加・削除を実現することができて、これだけ見ればハッシュテーブルのほうが優れているように見える。ところが現実の大規模なデータベースでは、ハッシュテーブルではなく木構造が使われることが多い。その理由を考えよ。
いくつか重要な理由があるので思いつくだけ書いてください！<br>

### 解答（思いついたこと）
理由１：ハッシュだと、O(1)にするためには再ハッシュ（resize）や他段階ハッシュが必要になるため、必要なメモリが動的に変化し、事前にどんなに大きな入力数でも扱える計算資源を十分に確保することが難しいから。
理由２：再ハッシュするか否か、の条件分岐やcollision対策が、ユースケースに最適化されていれば木構造と同等orより速いかもしれないが、そうでない場合は木構造に比べ、計算速度が劣るから（かもしれない？と思いました）。

## Challenge 3
### 問題
もっとも直近にアクセスされたページ上位 X 個をキャッシュしておく😊
アクセス系列が「A, A, A, A, B, A, C, D, D, B, B, D, B, E」ならば、「B, D, E」をキャッシュ
このようなキャッシュの管理をほぼ O(1) で実現できるデータ構造を考えてください！

### 解答（考えたこと）
ある（URL, Web page）セットにアクセスがあった時、URL と Web page をハッシュテーブルの bucket 住所 (e.g., 7) に格納する　のに加え、それとは別に1つリストを用意しておいて、そこに、bucket 住所（ここでは 7）を append していけば良いのではないかと考えた。
そうすれば、後ろから順にlist[-x:]すればO(1)で直近にアクセスのあった（URL, Web page）x個の bucket 住所がわかり、ハッシュテーブルでの検索もO(1)なので、全部でO(1)で処理できるのではないかと思った。

直近にアクセスがあった(URL, Web page)セットについて、ハッシュテーブル上での住所をその都度、リストの末尾に append していくのみで良いので、頻繁にアクセスする（後ろからX個以内のところにアクセス履歴のある）ページにアクセスした際の、情報アップデートにも対応できるのではないかと考えた。






