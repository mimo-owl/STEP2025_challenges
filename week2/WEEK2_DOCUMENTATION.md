# Documentation for Week2 Challenges
### Author: Mimo Shirasaka

## Challenge 1
最も良いプログラム：hash_table3.py（再ハッシュと （桁の考えで順番考慮した）collision対策入りの hash table）<br>
その他プログラム：hash_table.py（ベーシックな hash table）, hash_table2.py（再ハッシュと （順番考慮による）collision対策入りの hash table）<br>

* 以下の説明は hash_table3.pyについて書きます。
1. Delete()関数の実装
    1. マップ内の探す箱（bucket）の住所を算出：
        削除したい単語`key`に対し、ハッシュ値を計算。次にマップの長さ`bucket_size`で modをとる。
    2. 探す箱の中に`key`と一致する単語（アイテム）があるかを探す：

        - 見つかったら<br>
            -> 1つ前のマス`prev`が指すポインタ`prev.next`を、今いるマス`item`ではなく、今いる次のマス`item.next`を指すようにすることで今いるマスを実質的に削除(delete)する<br>
            -> `return True`<br>
        - 見つからなかったら<br>
            -> 次を確認<br>
        - 最後まで見つからなかったら<br>
            -> `return False`<br>
2. 再ハッシュの実装<br>
    1. 再ハッシュの条件をつける：<br>
        要素数がテーブルサイズの 70% を上回ったら、テーブルサイズを 約2倍（の奇数）に拡張、要素数がテーブルサイズの 30% を下回ったら、テーブルサイズを約半分（の奇数）に縮小<br>
    2. ハッシュテーブルを resize する：<br>
        新しいテーブル`new_buckets`を resize するサイズで用意し、元のテーブルを参照。bucketごとに見ていき、連結リストで繋がれた各アイテムについて`new_buckets`用の住所を算出する。
        各アイテムについて`Item()`を用いてリフォオーマットし、新しいテーブルに格納する。<br>
    3. 再ハッシュ済みの新しいテーブル`new_buckets`を、現在のテーブル`self.buckets`として定義する。<br>

    メモ：上記の実装でO(1)は実現したが、テーブルサイズの候補を、素数リストとして持っておくなどして、97 から始まり、70%を上回ったら次に大きい素数にリサイズする、30%を下回ったら次に小さい素数にリサイズする、というようにすると、常にテーブルサイズを素数にすることができる。なお、素数のリストをどれだけたくさん、どの範囲を中心に持っているべきか、ということについては議論の余地がある。無限大なので、全てを持っておくことはできないが、ある程度データ数の上限がわかっていれば、それに対して従分数の素数をリストに準備しておくことで対応可能なのではないかと考えた。

3. Collision 対策<br>
    サンプルコードにおけるハッシュ値算出の課題は、AliceとElicaなど、文字の組み合わせが同じものが存在する時に、bucket住所が同じになり、衝突(collision) が発生する、というものであった。
    サンプルコードでは、名前に含まれる文字のASCIIコードの和をハッシュ値として計算しており、文字の登場順番が考慮されていなかった。<br>
    そこで、Collision対策として、文字の登場順番を考慮したハッシュ値を算出すればより計算効率が速くなるのではないかと考えた。
    実装としては、数字「12」と「21」（10進数）が違う理由は、12 = 1 * 10^2 + 2、21 = 2 * 10^2 + 1 だから。ということを応用し、アルファベットが全部で26種類なので、26を底として「桁」のような考えを用いて、`\<文字のASCIIコード\> * 26 ** (乗数)` の和をハッシュ値とすることにより順番が違うと違う bucket住所になるようにした。<br>
    例："ab"は `\<"a"のASCIIコード\> * 26 ** 1 + \<"b"のASCIIコード\> `, "ba"は `\<"b"のASCIIコード\> * 26 ** 1 + \<"a"のASCIIコード\> `となるので、異なるハッシュ値を得ることがわかる。

## Challenge 2
### 問題
木構造を使えば O(log N)、ハッシュテーブルを使えばほぼ O(1) で検索・追加・削除を実現することができて、これだけ見ればハッシュテーブルのほうが優れているように見える。ところが現実の大規模なデータベースでは、ハッシュテーブルではなく木構造が使われることが多い。その理由を考えよ。
いくつか重要な理由があるので思いつくだけ書いてください！<br>

### 解答（思いついたこと）
理由１：ハッシュだと、O(1)にするためには再ハッシュ（resize）や他段階ハッシュが必要になるため、必要なメモリが動的に変化し、事前にどんなに大きな入力数でも扱える計算資源を十分に確保することが難しいから。<br>
理由２：再ハッシュするか否か、の条件分岐やcollision対策が、ユースケースに最適化されていれば木構造と同等orより速いかもしれないが、そうでない場合は木構造に比べ、計算速度が劣るから（かもしれない？と思いました）。
--> 平均すると計算時間が木の方が速い。（木は、常にO(logN)なのに対し、ハッシュは、最悪の場合O(N)になる。）<br>
理由３：多段階ハッシュはしたくないから。（多段階ハッシュになると、計算量が増え、O(1)で計算できなくなる）<br>
理由４：木構造だと範囲検索ができる（Cより小さいのを調べてください、などが、木なら左側を取ってくれば良いので速い。一方、ハッシュだと遅い。）

## Challenge 3
### 問題
もっとも直近にアクセスされたページ上位 X 個をキャッシュしておく😊
アクセス系列が「A, A, A, A, B, A, C, D, D, B, B, D, B, E」ならば、「B, D, E」をキャッシュ
このようなキャッシュの管理をほぼ O(1) で実現できるデータ構造を考えてください！

### 解答（考えたこと）
各アクセスに対して、key(URL), value(page), next（次のItemを指す）が Item() として定められる。
ここに、prevという要素を足して各Itemをハッシュテーブルで保存していくことで、next が None である Item を起点として、prev ポインタを使って遡っていくことで、アクセスのあった直近X個のItem(URL, page)をO(1)で見つけられるのではないかと考えた。
prev, next を用いると、同じページに再度アクセスがあったときにも、prev, next ポインタを変えることで連結を繋ぎかえられるので、この問題に適切なアルゴリズムなのではないかと考えた。　

メンターさんからのヒントのメモ：Item() で、key, value, next の他に、もう一つ何か使うことで、同じページにアクセスがあっても、うまくremove して、情報を更新できるようになる。






